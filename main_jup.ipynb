{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "#from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
    "from torch_geometric.nn.pool.connect.filter_edges import filter_adj\n",
    "from torch_geometric.nn.pool.select.topk import topk\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "from torch_sparse import spspmm, coalesce\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torch.autograd import Function\n",
    "from torch_scatter import scatter_add, scatter_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "dataset_path = '/dataset'\n",
    "output_path = './output'\n",
    "\n",
    "device = 0 #set to -1 if using only cpu\n",
    "dataset = \"DD\" # [\"DD\", \"PROTEINS\", \"NCI1\", \"NCI109\", \"Mutagenicity\", \"ENZYMES\"]\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "weight_decay = 0.001\n",
    "batch_size = 64\n",
    "pooling_ratio = 0.3\n",
    "dropout_ratio = 0.5\n",
    "conv_layers = 2\n",
    "\n",
    "nhid = 128\n",
    "sample_neighbor = False # use sample method\n",
    "seed = 777\n",
    "sparse_attention = True\n",
    "structure_learning = False\n",
    "lamb = 1.0 #trade-off parameter\n",
    "epochs = 1000 #max number of training epochs \n",
    "patience = 100 #patience for early stopping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Sparse Model of Attention and Multi-Label Classification (Marins & Astudillo, 2016)</h3>\n",
    "Modifications made: Making it work at scatter operation scenarios ex: calculating softmax according to batch indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_sort(x, batch, fill_value=-1e16):\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
    "    dense_x[index] = x\n",
    "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
    "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
    "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
    "\n",
    "    sorted_x = sorted_x.view(-1)\n",
    "    filled_index = sorted_x != fill_value\n",
    "\n",
    "    sorted_x = sorted_x[filled_index]\n",
    "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
    "\n",
    "    return sorted_x, cumsum_sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_ix_like(batch):\n",
    "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
    "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
    "    idx = torch.cat(idx, dim=0)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _threshold_and_support(x, batch):\n",
    "\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
    "    input_cumsum = input_cumsum - 1.0\n",
    "    rhos = _make_ix_like(batch).to(x.dtype)\n",
    "    support = rhos * sorted_input > input_cumsum\n",
    "\n",
    "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
    "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
    "    idx = support_size + cum_num_nodes - 1\n",
    "    mask = idx < 0\n",
    "    idx[mask] = 0\n",
    "    tau = input_cumsum.gather(0, idx)\n",
    "    tau /= support_size.to(x.dtype)\n",
    "\n",
    "    return tau, support_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsemaxFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, batch):\n",
    "        max_val, _ = scatter_max(x, batch)\n",
    "        x -= max_val[batch]\n",
    "        tau, supp_size = _threshold_and_support(x, batch)\n",
    "        output = torch.clamp(x - tau[batch], min=0)\n",
    "        ctx.save_for_backward(supp_size, output, batch)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output, batch = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        return sparsemax(x, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Layers for Architecture</h3>\n",
    "\n",
    "With the addition of TwoHopNeighborhood function, deals with the absence of autograd support (python_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''extending edge set of graph to include edges that represent two-hop neighborhoods'''\n",
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min')\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implementation of GCN layer. Uses message-passing paradigm to aggregate information from node's neighbors.'''\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "    #resets cache\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    #normalization\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GNN layer designed to compute node info score across graph using message passing'''\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''hierarchical graph pooling layer with structure learning '''\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.8, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.5. Utilization of Cuda for fast computation. </h3>\n",
    "\n",
    "Retrieval of dataset is also done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/DD.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(device)\n",
    "\n",
    "dataset = TUDataset(os.path.join('data', dataset), name=dataset, use_node_attr=True)\n",
    "\n",
    "num_classes = dataset.num_classes\n",
    "num_features = dataset.num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Model Architecture</h3>\n",
    "    Utilizes GCN for feature extraction.<br>\n",
    "    HGPSLPool for graph coarsening and structure optimization. <br>\n",
    "    fully connected layers for classification.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''NN architecture designed for graph-based data. \n",
    "'''\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.nhid = nhid\n",
    "        self.num_classes = num_classes\n",
    "        self.pooling_ratio = pooling_ratio\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.sample = sample_neighbor\n",
    "        self.sparse = sparse_attention\n",
    "        self.sl = structure_learning\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "        self.conv2 = GCN(self.nhid, self.nhid)\n",
    "        self.conv3 = GCN(self.nhid, self.nhid)\n",
    "\n",
    "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
    "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_attr = None\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\miniconda3\\envs\\hgp\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "num_training = int(len(dataset) * 0.8)\n",
    "num_val = int(len(dataset) * 0.1)\n",
    "num_test = len(dataset) - (num_training + num_val)\n",
    "training_set, validation_set, test_set = random_split(dataset, [num_training, num_val, num_test])\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Training and computation of prediction accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test(loader):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    loss_test = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        loss_test += F.nll_loss(out, data.y).item()\n",
    "    return correct / len(loader.dataset), loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    min_loss = 1e10\n",
    "    patience_cnt = 0\n",
    "    val_loss_values = []\n",
    "    best_epoch = 0\n",
    "\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loss_train = 0.0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "        acc_train = correct / len(train_loader.dataset)\n",
    "        acc_val, loss_val = compute_test(val_loader)\n",
    "        print('Epoch: {:04d}'.format(epoch + 1), 'loss_train: {:.6f}'.format(loss_train),\n",
    "              'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
    "              'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
    "\n",
    "        val_loss_values.append(loss_val)\n",
    "        torch.save(model.state_dict(), '{}.pth'.format(epoch))\n",
    "        if val_loss_values[-1] < min_loss:\n",
    "            min_loss = val_loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "\n",
    "        if patience_cnt == patience:\n",
    "            break\n",
    "\n",
    "        files = glob.glob('*.pth')\n",
    "        for f in files:\n",
    "            epoch_nb = int(f.split('.')[0])\n",
    "            if epoch_nb < best_epoch:\n",
    "                os.remove(f)\n",
    "\n",
    "    files = glob.glob('*.pth')\n",
    "    for f in files:\n",
    "        epoch_nb = int(f.split('.')[0])\n",
    "        if epoch_nb > best_epoch:\n",
    "            os.remove(f)\n",
    "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))\n",
    "\n",
    "    return best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 10.407462 acc_train: 0.487261 loss_val: 1.378409 acc_val: 0.641026 time: 0.736602s\n",
      "Epoch: 0002 loss_train: 10.293732 acc_train: 0.597665 loss_val: 1.364085 acc_val: 0.615385 time: 1.115649s\n",
      "Epoch: 0003 loss_train: 10.205181 acc_train: 0.587049 loss_val: 1.348846 acc_val: 0.615385 time: 1.503698s\n",
      "Epoch: 0004 loss_train: 10.124473 acc_train: 0.587049 loss_val: 1.334796 acc_val: 0.615385 time: 1.909374s\n",
      "Epoch: 0005 loss_train: 10.054274 acc_train: 0.587049 loss_val: 1.326087 acc_val: 0.615385 time: 2.310010s\n",
      "Epoch: 0006 loss_train: 10.015880 acc_train: 0.587049 loss_val: 1.319650 acc_val: 0.615385 time: 2.711568s\n",
      "Epoch: 0007 loss_train: 10.004140 acc_train: 0.587049 loss_val: 1.315426 acc_val: 0.615385 time: 3.127871s\n",
      "Epoch: 0008 loss_train: 9.975259 acc_train: 0.587049 loss_val: 1.314694 acc_val: 0.615385 time: 3.509753s\n",
      "Epoch: 0009 loss_train: 9.967885 acc_train: 0.587049 loss_val: 1.312974 acc_val: 0.615385 time: 3.878889s\n",
      "Epoch: 0010 loss_train: 9.941534 acc_train: 0.587049 loss_val: 1.311704 acc_val: 0.615385 time: 4.251372s\n",
      "Epoch: 0011 loss_train: 9.928373 acc_train: 0.587049 loss_val: 1.310165 acc_val: 0.615385 time: 4.634917s\n",
      "Epoch: 0012 loss_train: 9.913594 acc_train: 0.587049 loss_val: 1.307816 acc_val: 0.615385 time: 5.020967s\n",
      "Epoch: 0013 loss_train: 9.891008 acc_train: 0.587049 loss_val: 1.306186 acc_val: 0.615385 time: 5.390398s\n",
      "Epoch: 0014 loss_train: 9.883793 acc_train: 0.587049 loss_val: 1.302427 acc_val: 0.615385 time: 5.778732s\n",
      "Epoch: 0015 loss_train: 9.855670 acc_train: 0.587049 loss_val: 1.300707 acc_val: 0.615385 time: 6.171344s\n",
      "Epoch: 0016 loss_train: 9.815795 acc_train: 0.587049 loss_val: 1.299489 acc_val: 0.615385 time: 6.559765s\n",
      "Epoch: 0017 loss_train: 9.787240 acc_train: 0.587049 loss_val: 1.295274 acc_val: 0.615385 time: 6.929822s\n",
      "Epoch: 0018 loss_train: 9.774267 acc_train: 0.587049 loss_val: 1.291710 acc_val: 0.615385 time: 7.304700s\n",
      "Epoch: 0019 loss_train: 9.725955 acc_train: 0.590234 loss_val: 1.292957 acc_val: 0.615385 time: 7.665289s\n",
      "Epoch: 0020 loss_train: 9.703403 acc_train: 0.589172 loss_val: 1.283401 acc_val: 0.615385 time: 8.027749s\n",
      "Epoch: 0021 loss_train: 9.617941 acc_train: 0.599788 loss_val: 1.283755 acc_val: 0.623932 time: 8.385385s\n",
      "Epoch: 0022 loss_train: 9.560394 acc_train: 0.602972 loss_val: 1.274978 acc_val: 0.623932 time: 8.760087s\n",
      "Epoch: 0023 loss_train: 9.505810 acc_train: 0.609342 loss_val: 1.270195 acc_val: 0.623932 time: 9.125627s\n",
      "Epoch: 0024 loss_train: 9.409008 acc_train: 0.631635 loss_val: 1.266906 acc_val: 0.632479 time: 9.500674s\n",
      "Epoch: 0025 loss_train: 9.339405 acc_train: 0.656051 loss_val: 1.257395 acc_val: 0.606838 time: 9.870542s\n",
      "Epoch: 0026 loss_train: 9.213852 acc_train: 0.677282 loss_val: 1.245841 acc_val: 0.649573 time: 10.242661s\n",
      "Epoch: 0027 loss_train: 9.079184 acc_train: 0.667728 loss_val: 1.233445 acc_val: 0.649573 time: 10.617095s\n",
      "Epoch: 0028 loss_train: 8.965445 acc_train: 0.684713 loss_val: 1.236987 acc_val: 0.717949 time: 10.997923s\n",
      "Epoch: 0029 loss_train: 8.961397 acc_train: 0.730361 loss_val: 1.223381 acc_val: 0.641026 time: 11.376212s\n",
      "Epoch: 0030 loss_train: 8.727689 acc_train: 0.722930 loss_val: 1.202208 acc_val: 0.709402 time: 11.727443s\n",
      "Epoch: 0031 loss_train: 8.604020 acc_train: 0.725053 loss_val: 1.189522 acc_val: 0.717949 time: 12.078922s\n",
      "Epoch: 0032 loss_train: 8.429194 acc_train: 0.757962 loss_val: 1.179075 acc_val: 0.709402 time: 12.451469s\n",
      "Epoch: 0033 loss_train: 8.257816 acc_train: 0.764331 loss_val: 1.161461 acc_val: 0.709402 time: 12.844698s\n",
      "Epoch: 0034 loss_train: 8.110670 acc_train: 0.768577 loss_val: 1.147107 acc_val: 0.709402 time: 13.248757s\n",
      "Epoch: 0035 loss_train: 8.001370 acc_train: 0.764331 loss_val: 1.142709 acc_val: 0.700855 time: 13.633809s\n",
      "Epoch: 0036 loss_train: 7.896888 acc_train: 0.774947 loss_val: 1.158071 acc_val: 0.692308 time: 14.004419s\n",
      "Epoch: 0037 loss_train: 7.831012 acc_train: 0.769639 loss_val: 1.106369 acc_val: 0.735043 time: 14.353474s\n",
      "Epoch: 0038 loss_train: 7.700775 acc_train: 0.773885 loss_val: 1.104662 acc_val: 0.786325 time: 14.727526s\n",
      "Epoch: 0039 loss_train: 7.688868 acc_train: 0.777070 loss_val: 1.086556 acc_val: 0.735043 time: 15.131186s\n",
      "Epoch: 0040 loss_train: 7.536519 acc_train: 0.781316 loss_val: 1.072470 acc_val: 0.735043 time: 16.184573s\n",
      "Epoch: 0041 loss_train: 7.457981 acc_train: 0.781316 loss_val: 1.078450 acc_val: 0.735043 time: 16.572644s\n",
      "Epoch: 0042 loss_train: 7.431051 acc_train: 0.780255 loss_val: 1.102380 acc_val: 0.726496 time: 16.974239s\n",
      "Epoch: 0043 loss_train: 7.483196 acc_train: 0.769639 loss_val: 1.064298 acc_val: 0.769231 time: 17.377033s\n",
      "Epoch: 0044 loss_train: 7.298271 acc_train: 0.777070 loss_val: 1.054340 acc_val: 0.786325 time: 17.747012s\n",
      "Epoch: 0045 loss_train: 7.309994 acc_train: 0.785563 loss_val: 1.060112 acc_val: 0.769231 time: 18.105429s\n",
      "Epoch: 0046 loss_train: 7.273505 acc_train: 0.781316 loss_val: 1.077583 acc_val: 0.743590 time: 18.477475s\n",
      "Epoch: 0047 loss_train: 7.270893 acc_train: 0.783439 loss_val: 1.075835 acc_val: 0.752137 time: 18.843026s\n",
      "Epoch: 0048 loss_train: 7.255246 acc_train: 0.779193 loss_val: 1.077495 acc_val: 0.769231 time: 19.219079s\n",
      "Epoch: 0049 loss_train: 7.237174 acc_train: 0.786624 loss_val: 1.098594 acc_val: 0.752137 time: 19.599642s\n",
      "Epoch: 0050 loss_train: 7.146091 acc_train: 0.795117 loss_val: 1.083491 acc_val: 0.760684 time: 19.976723s\n",
      "Epoch: 0051 loss_train: 7.174464 acc_train: 0.788747 loss_val: 1.079557 acc_val: 0.777778 time: 20.352271s\n",
      "Epoch: 0052 loss_train: 7.060486 acc_train: 0.786624 loss_val: 1.093138 acc_val: 0.743590 time: 20.727898s\n",
      "Epoch: 0053 loss_train: 7.038386 acc_train: 0.785563 loss_val: 1.061774 acc_val: 0.777778 time: 21.097249s\n",
      "Epoch: 0054 loss_train: 6.979596 acc_train: 0.786624 loss_val: 1.067907 acc_val: 0.777778 time: 21.463849s\n",
      "Epoch: 0055 loss_train: 6.913121 acc_train: 0.797240 loss_val: 1.135676 acc_val: 0.726496 time: 21.831388s\n",
      "Epoch: 0056 loss_train: 6.973924 acc_train: 0.789809 loss_val: 1.072982 acc_val: 0.760684 time: 22.220980s\n",
      "Epoch: 0057 loss_train: 6.964735 acc_train: 0.782378 loss_val: 1.079583 acc_val: 0.743590 time: 22.595545s\n",
      "Epoch: 0058 loss_train: 6.938984 acc_train: 0.796178 loss_val: 1.081348 acc_val: 0.735043 time: 22.960887s\n",
      "Epoch: 0059 loss_train: 6.851894 acc_train: 0.792994 loss_val: 1.083008 acc_val: 0.752137 time: 23.318035s\n",
      "Epoch: 0060 loss_train: 6.911931 acc_train: 0.781316 loss_val: 1.097244 acc_val: 0.743590 time: 23.673075s\n",
      "Epoch: 0061 loss_train: 6.805484 acc_train: 0.797240 loss_val: 1.094896 acc_val: 0.726496 time: 24.030614s\n",
      "Epoch: 0062 loss_train: 6.794694 acc_train: 0.794055 loss_val: 1.105783 acc_val: 0.752137 time: 24.407333s\n",
      "Epoch: 0063 loss_train: 6.752059 acc_train: 0.781316 loss_val: 1.105346 acc_val: 0.717949 time: 24.765388s\n",
      "Epoch: 0064 loss_train: 6.772211 acc_train: 0.792994 loss_val: 1.092947 acc_val: 0.752137 time: 25.131931s\n",
      "Epoch: 0065 loss_train: 6.742832 acc_train: 0.792994 loss_val: 1.089020 acc_val: 0.743590 time: 25.499595s\n",
      "Epoch: 0066 loss_train: 6.665404 acc_train: 0.788747 loss_val: 1.084941 acc_val: 0.735043 time: 25.859925s\n",
      "Epoch: 0067 loss_train: 6.645130 acc_train: 0.800425 loss_val: 1.082314 acc_val: 0.735043 time: 26.246323s\n",
      "Epoch: 0068 loss_train: 6.631465 acc_train: 0.794055 loss_val: 1.083605 acc_val: 0.717949 time: 26.606441s\n",
      "Epoch: 0069 loss_train: 6.676034 acc_train: 0.798301 loss_val: 1.072624 acc_val: 0.735043 time: 26.962666s\n",
      "Epoch: 0070 loss_train: 6.655085 acc_train: 0.799363 loss_val: 1.059783 acc_val: 0.752137 time: 27.338114s\n",
      "Epoch: 0071 loss_train: 6.626330 acc_train: 0.799363 loss_val: 1.068782 acc_val: 0.777778 time: 27.697703s\n",
      "Epoch: 0072 loss_train: 6.640256 acc_train: 0.802548 loss_val: 1.082675 acc_val: 0.760684 time: 28.065307s\n",
      "Epoch: 0073 loss_train: 6.566255 acc_train: 0.808917 loss_val: 1.053928 acc_val: 0.760684 time: 28.438854s\n",
      "Epoch: 0074 loss_train: 6.526374 acc_train: 0.801486 loss_val: 1.055079 acc_val: 0.760684 time: 28.833432s\n",
      "Epoch: 0075 loss_train: 6.616572 acc_train: 0.797240 loss_val: 1.088532 acc_val: 0.786325 time: 29.300841s\n",
      "Epoch: 0076 loss_train: 6.504993 acc_train: 0.805732 loss_val: 1.098694 acc_val: 0.760684 time: 29.693823s\n",
      "Epoch: 0077 loss_train: 6.647923 acc_train: 0.801486 loss_val: 1.091649 acc_val: 0.769231 time: 30.066413s\n",
      "Epoch: 0078 loss_train: 6.647317 acc_train: 0.792994 loss_val: 1.080903 acc_val: 0.752137 time: 30.441549s\n",
      "Epoch: 0079 loss_train: 6.486171 acc_train: 0.804671 loss_val: 1.085295 acc_val: 0.786325 time: 30.818783s\n",
      "Epoch: 0080 loss_train: 6.572280 acc_train: 0.804671 loss_val: 1.058924 acc_val: 0.752137 time: 31.180387s\n",
      "Epoch: 0081 loss_train: 6.454799 acc_train: 0.804671 loss_val: 1.065619 acc_val: 0.777778 time: 31.568975s\n",
      "Epoch: 0082 loss_train: 6.471579 acc_train: 0.797240 loss_val: 1.100071 acc_val: 0.769231 time: 31.954285s\n",
      "Epoch: 0083 loss_train: 6.449386 acc_train: 0.802548 loss_val: 1.145442 acc_val: 0.786325 time: 32.315922s\n",
      "Epoch: 0084 loss_train: 6.386710 acc_train: 0.801486 loss_val: 1.088247 acc_val: 0.743590 time: 32.707025s\n",
      "Epoch: 0085 loss_train: 6.362590 acc_train: 0.805732 loss_val: 1.079933 acc_val: 0.743590 time: 33.069664s\n",
      "Epoch: 0086 loss_train: 6.349133 acc_train: 0.806794 loss_val: 1.099150 acc_val: 0.777778 time: 33.447724s\n",
      "Epoch: 0087 loss_train: 6.349824 acc_train: 0.807856 loss_val: 1.104764 acc_val: 0.743590 time: 33.831664s\n",
      "Epoch: 0088 loss_train: 6.314982 acc_train: 0.808917 loss_val: 1.153164 acc_val: 0.777778 time: 34.240746s\n",
      "Epoch: 0089 loss_train: 6.356294 acc_train: 0.809979 loss_val: 1.131665 acc_val: 0.769231 time: 34.610807s\n",
      "Epoch: 0090 loss_train: 6.321438 acc_train: 0.807856 loss_val: 1.088242 acc_val: 0.735043 time: 34.995100s\n",
      "Epoch: 0091 loss_train: 6.308699 acc_train: 0.818471 loss_val: 1.112038 acc_val: 0.760684 time: 35.366176s\n",
      "Epoch: 0092 loss_train: 6.254858 acc_train: 0.818471 loss_val: 1.122396 acc_val: 0.752137 time: 35.732238s\n",
      "Epoch: 0093 loss_train: 6.203379 acc_train: 0.817410 loss_val: 1.106526 acc_val: 0.760684 time: 36.114292s\n",
      "Epoch: 0094 loss_train: 6.196550 acc_train: 0.813163 loss_val: 1.109499 acc_val: 0.760684 time: 36.483466s\n",
      "Epoch: 0095 loss_train: 6.175690 acc_train: 0.809979 loss_val: 1.108070 acc_val: 0.777778 time: 36.851530s\n",
      "Epoch: 0096 loss_train: 6.185893 acc_train: 0.812102 loss_val: 1.109477 acc_val: 0.760684 time: 37.200107s\n",
      "Epoch: 0097 loss_train: 6.182336 acc_train: 0.813163 loss_val: 1.119925 acc_val: 0.735043 time: 37.584162s\n",
      "Epoch: 0098 loss_train: 6.352577 acc_train: 0.809979 loss_val: 1.113652 acc_val: 0.743590 time: 37.952600s\n",
      "Epoch: 0099 loss_train: 6.207692 acc_train: 0.812102 loss_val: 1.154259 acc_val: 0.752137 time: 38.324608s\n",
      "Epoch: 0100 loss_train: 6.248492 acc_train: 0.807856 loss_val: 1.125980 acc_val: 0.777778 time: 38.686663s\n",
      "Epoch: 0101 loss_train: 6.103738 acc_train: 0.815287 loss_val: 1.101660 acc_val: 0.760684 time: 39.047209s\n",
      "Epoch: 0102 loss_train: 6.097718 acc_train: 0.816348 loss_val: 1.111700 acc_val: 0.760684 time: 39.424268s\n",
      "Epoch: 0103 loss_train: 6.093289 acc_train: 0.814225 loss_val: 1.122157 acc_val: 0.760684 time: 39.851012s\n",
      "Epoch: 0104 loss_train: 6.115787 acc_train: 0.816348 loss_val: 1.120531 acc_val: 0.752137 time: 40.229611s\n",
      "Epoch: 0105 loss_train: 6.146823 acc_train: 0.808917 loss_val: 1.092344 acc_val: 0.769231 time: 40.615678s\n",
      "Epoch: 0106 loss_train: 6.009679 acc_train: 0.816348 loss_val: 1.092550 acc_val: 0.769231 time: 40.990902s\n",
      "Epoch: 0107 loss_train: 6.002266 acc_train: 0.817410 loss_val: 1.084968 acc_val: 0.786325 time: 41.395678s\n",
      "Epoch: 0108 loss_train: 5.964676 acc_train: 0.817410 loss_val: 1.104408 acc_val: 0.769231 time: 41.816733s\n",
      "Epoch: 0109 loss_train: 5.980138 acc_train: 0.818471 loss_val: 1.090441 acc_val: 0.769231 time: 42.221796s\n",
      "Epoch: 0110 loss_train: 6.038592 acc_train: 0.814225 loss_val: 1.081796 acc_val: 0.777778 time: 42.600364s\n",
      "Epoch: 0111 loss_train: 5.965514 acc_train: 0.819533 loss_val: 1.106334 acc_val: 0.769231 time: 42.966937s\n",
      "Epoch: 0112 loss_train: 6.012191 acc_train: 0.815287 loss_val: 1.076845 acc_val: 0.777778 time: 43.347993s\n",
      "Epoch: 0113 loss_train: 6.026615 acc_train: 0.814225 loss_val: 1.081728 acc_val: 0.777778 time: 43.698651s\n",
      "Epoch: 0114 loss_train: 5.913621 acc_train: 0.826964 loss_val: 1.080626 acc_val: 0.777778 time: 44.073215s\n",
      "Epoch: 0115 loss_train: 5.905015 acc_train: 0.826964 loss_val: 1.105116 acc_val: 0.777778 time: 44.443292s\n",
      "Epoch: 0116 loss_train: 5.948676 acc_train: 0.823779 loss_val: 1.101590 acc_val: 0.777778 time: 44.801352s\n",
      "Epoch: 0117 loss_train: 5.880893 acc_train: 0.820594 loss_val: 1.177746 acc_val: 0.743590 time: 45.157440s\n",
      "Epoch: 0118 loss_train: 6.012020 acc_train: 0.817410 loss_val: 1.083425 acc_val: 0.777778 time: 45.519504s\n",
      "Epoch: 0119 loss_train: 5.866446 acc_train: 0.826964 loss_val: 1.095700 acc_val: 0.777778 time: 45.894052s\n",
      "Epoch: 0120 loss_train: 5.848765 acc_train: 0.820594 loss_val: 1.090072 acc_val: 0.769231 time: 46.268103s\n",
      "Epoch: 0121 loss_train: 5.892111 acc_train: 0.820594 loss_val: 1.095246 acc_val: 0.786325 time: 46.641636s\n",
      "Epoch: 0122 loss_train: 5.785700 acc_train: 0.830149 loss_val: 1.101822 acc_val: 0.786325 time: 47.043338s\n",
      "Epoch: 0123 loss_train: 5.966311 acc_train: 0.813163 loss_val: 1.097406 acc_val: 0.786325 time: 47.416392s\n",
      "Epoch: 0124 loss_train: 5.986331 acc_train: 0.823779 loss_val: 1.108518 acc_val: 0.794872 time: 47.780984s\n",
      "Epoch: 0125 loss_train: 5.879754 acc_train: 0.829087 loss_val: 1.094408 acc_val: 0.769231 time: 48.162587s\n",
      "Epoch: 0126 loss_train: 5.938228 acc_train: 0.821656 loss_val: 1.131499 acc_val: 0.735043 time: 48.550165s\n",
      "Epoch: 0127 loss_train: 5.945413 acc_train: 0.822718 loss_val: 1.108724 acc_val: 0.760684 time: 48.904734s\n",
      "Epoch: 0128 loss_train: 5.779928 acc_train: 0.835456 loss_val: 1.103687 acc_val: 0.794872 time: 49.287611s\n",
      "Epoch: 0129 loss_train: 5.768388 acc_train: 0.831210 loss_val: 1.099439 acc_val: 0.769231 time: 49.664670s\n",
      "Epoch: 0130 loss_train: 5.853242 acc_train: 0.825902 loss_val: 1.113903 acc_val: 0.769231 time: 50.032722s\n",
      "Epoch: 0131 loss_train: 5.798448 acc_train: 0.835456 loss_val: 1.131974 acc_val: 0.743590 time: 50.388780s\n",
      "Epoch: 0132 loss_train: 5.863741 acc_train: 0.832272 loss_val: 1.101111 acc_val: 0.777778 time: 50.781025s\n",
      "Epoch: 0133 loss_train: 5.930819 acc_train: 0.822718 loss_val: 1.106765 acc_val: 0.752137 time: 51.154134s\n",
      "Epoch: 0134 loss_train: 5.720278 acc_train: 0.830149 loss_val: 1.101664 acc_val: 0.743590 time: 51.522368s\n",
      "Epoch: 0135 loss_train: 5.843228 acc_train: 0.820594 loss_val: 1.117656 acc_val: 0.786325 time: 51.896425s\n",
      "Epoch: 0136 loss_train: 5.615256 acc_train: 0.839703 loss_val: 1.109450 acc_val: 0.769231 time: 52.290815s\n",
      "Epoch: 0137 loss_train: 5.595422 acc_train: 0.839703 loss_val: 1.090502 acc_val: 0.786325 time: 52.666390s\n",
      "Epoch: 0138 loss_train: 5.600514 acc_train: 0.835456 loss_val: 1.148658 acc_val: 0.709402 time: 53.032027s\n",
      "Epoch: 0139 loss_train: 5.786144 acc_train: 0.832272 loss_val: 1.069621 acc_val: 0.769231 time: 53.401147s\n",
      "Epoch: 0140 loss_train: 5.593496 acc_train: 0.842887 loss_val: 1.067707 acc_val: 0.769231 time: 53.783593s\n",
      "Epoch: 0141 loss_train: 5.474342 acc_train: 0.846072 loss_val: 1.088295 acc_val: 0.777778 time: 54.146706s\n",
      "Epoch: 0142 loss_train: 5.460802 acc_train: 0.840764 loss_val: 1.098276 acc_val: 0.769231 time: 54.518366s\n",
      "Epoch: 0143 loss_train: 5.576804 acc_train: 0.836518 loss_val: 1.090458 acc_val: 0.769231 time: 54.870438s\n",
      "Epoch: 0144 loss_train: 5.574490 acc_train: 0.841826 loss_val: 1.108605 acc_val: 0.777778 time: 55.258982s\n",
      "Epoch: 0145 loss_train: 5.484018 acc_train: 0.848195 loss_val: 1.088732 acc_val: 0.777778 time: 55.614768s\n",
      "Epoch: 0146 loss_train: 5.449039 acc_train: 0.845011 loss_val: 1.087183 acc_val: 0.769231 time: 55.984403s\n",
      "Epoch: 0147 loss_train: 5.515026 acc_train: 0.839703 loss_val: 1.091618 acc_val: 0.777778 time: 56.345947s\n",
      "Epoch: 0148 loss_train: 5.452165 acc_train: 0.847134 loss_val: 1.076198 acc_val: 0.743590 time: 56.697947s\n",
      "Epoch: 0149 loss_train: 5.483747 acc_train: 0.850318 loss_val: 1.063988 acc_val: 0.777778 time: 57.052047s\n",
      "Epoch: 0150 loss_train: 5.362621 acc_train: 0.855626 loss_val: 1.071448 acc_val: 0.760684 time: 57.441101s\n",
      "Epoch: 0151 loss_train: 5.321100 acc_train: 0.857749 loss_val: 1.070995 acc_val: 0.760684 time: 57.825742s\n",
      "Epoch: 0152 loss_train: 5.438693 acc_train: 0.849257 loss_val: 1.071879 acc_val: 0.752137 time: 58.239085s\n",
      "Epoch: 0153 loss_train: 5.372040 acc_train: 0.846072 loss_val: 1.067807 acc_val: 0.752137 time: 58.718602s\n",
      "Epoch: 0154 loss_train: 5.342566 acc_train: 0.853503 loss_val: 1.080833 acc_val: 0.769231 time: 59.156658s\n",
      "Epoch: 0155 loss_train: 5.305506 acc_train: 0.847134 loss_val: 1.074080 acc_val: 0.777778 time: 59.573230s\n",
      "Epoch: 0156 loss_train: 5.341584 acc_train: 0.848195 loss_val: 1.082533 acc_val: 0.769231 time: 59.981285s\n",
      "Epoch: 0157 loss_train: 5.300536 acc_train: 0.856688 loss_val: 1.067453 acc_val: 0.777778 time: 60.430355s\n",
      "Epoch: 0158 loss_train: 5.259218 acc_train: 0.857749 loss_val: 1.064978 acc_val: 0.752137 time: 60.846026s\n",
      "Epoch: 0159 loss_train: 5.404609 acc_train: 0.831210 loss_val: 1.094875 acc_val: 0.709402 time: 61.283535s\n",
      "Epoch: 0160 loss_train: 5.216156 acc_train: 0.853503 loss_val: 1.060417 acc_val: 0.760684 time: 61.694105s\n",
      "Epoch: 0161 loss_train: 5.203000 acc_train: 0.858811 loss_val: 1.061050 acc_val: 0.760684 time: 62.129074s\n",
      "Epoch: 0162 loss_train: 5.149650 acc_train: 0.861996 loss_val: 1.067606 acc_val: 0.777778 time: 62.576654s\n",
      "Epoch: 0163 loss_train: 5.177294 acc_train: 0.854565 loss_val: 1.075668 acc_val: 0.777778 time: 63.038241s\n",
      "Epoch: 0164 loss_train: 5.189949 acc_train: 0.858811 loss_val: 1.127440 acc_val: 0.752137 time: 63.495820s\n",
      "Epoch: 0165 loss_train: 5.215300 acc_train: 0.851380 loss_val: 1.057255 acc_val: 0.786325 time: 63.900867s\n",
      "Epoch: 0166 loss_train: 5.064091 acc_train: 0.865180 loss_val: 1.081106 acc_val: 0.769231 time: 64.283918s\n",
      "Epoch: 0167 loss_train: 5.079479 acc_train: 0.864119 loss_val: 1.091594 acc_val: 0.777778 time: 64.693529s\n",
      "Epoch: 0168 loss_train: 5.040066 acc_train: 0.869427 loss_val: 1.088558 acc_val: 0.760684 time: 65.103103s\n",
      "Epoch: 0169 loss_train: 5.076037 acc_train: 0.864119 loss_val: 1.077194 acc_val: 0.760684 time: 65.527324s\n",
      "Epoch: 0170 loss_train: 5.149683 acc_train: 0.854565 loss_val: 1.074114 acc_val: 0.769231 time: 65.948928s\n",
      "Epoch: 0171 loss_train: 5.030918 acc_train: 0.872611 loss_val: 1.084652 acc_val: 0.769231 time: 66.376571s\n",
      "Epoch: 0172 loss_train: 5.033729 acc_train: 0.866242 loss_val: 1.076215 acc_val: 0.760684 time: 66.782640s\n",
      "Epoch: 0173 loss_train: 4.973537 acc_train: 0.867304 loss_val: 1.101933 acc_val: 0.709402 time: 67.200344s\n",
      "Optimization Finished! Total time elapsed: 67.299855\n",
      "Test set results, loss = 0.950088, accuracy = 0.781513\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Model training\n",
    "    best_model = train()\n",
    "    # Restore best model for test set\n",
    "    model.load_state_dict(torch.load('{}.pth'.format(best_model)))\n",
    "    test_acc, test_loss = compute_test(test_loader)\n",
    "    print('Test set results, loss = {:.6f}, accuracy = {:.6f}'.format(test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

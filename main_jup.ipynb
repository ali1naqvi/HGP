{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "#from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
    "from torch_geometric.nn.pool.connect.filter_edges import filter_adj\n",
    "from torch_geometric.nn.pool.select.topk import topk\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "from torch_sparse import spspmm, coalesce\n",
    "\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torch.autograd import Function\n",
    "from torch_scatter import scatter_add, scatter_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "dataset_path = '/dataset'\n",
    "output_path = './output'\n",
    "\n",
    "device = 0 #set to -1 if using only cpu\n",
    "dataset = \"DD\" # [\"DD\", \"PROTEINS\", \"NCI1\", \"NCI109\", \"Mutagenicity\", \"ENZYMES\"]\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "weight_decay = 0.001\n",
    "batch_size = 64\n",
    "pooling_ratio = 0.3\n",
    "dropout_ratio = 0.5\n",
    "conv_layers = 2\n",
    "\n",
    "nhid = 128\n",
    "sample_neighbor = False # use sample method\n",
    "seed = 777\n",
    "sparse_attention = True\n",
    "structure_learning = False\n",
    "lamb = 1.0 #trade-off parameter\n",
    "epochs = 1000 #max number of training epochs \n",
    "patience = 100 #patience for early stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_softmax\n",
    "def scatter_sort(x, batch, fill_value=-1e16):\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
    "    dense_x[index] = x\n",
    "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
    "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
    "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
    "\n",
    "    sorted_x = sorted_x.view(-1)\n",
    "    filled_index = sorted_x != fill_value\n",
    "\n",
    "    sorted_x = sorted_x[filled_index]\n",
    "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
    "\n",
    "    return sorted_x, cumsum_sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse softmax\n",
    "def _make_ix_like(batch):\n",
    "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
    "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
    "    idx = torch.cat(idx, dim=0)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse softmax\n",
    "def _threshold_and_support(x, batch):\n",
    "    \"\"\"Sparsemax building block: compute the threshold\n",
    "    Args:\n",
    "        x: input tensor to apply the sparsemax\n",
    "        batch: group indicators\n",
    "    Returns:\n",
    "        the threshold value\n",
    "    \"\"\"\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
    "    input_cumsum = input_cumsum - 1.0\n",
    "    rhos = _make_ix_like(batch).to(x.dtype)\n",
    "    support = rhos * sorted_input > input_cumsum\n",
    "\n",
    "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
    "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
    "    idx = support_size + cum_num_nodes - 1\n",
    "    mask = idx < 0\n",
    "    idx[mask] = 0\n",
    "    tau = input_cumsum.gather(0, idx)\n",
    "    tau /= support_size.to(x.dtype)\n",
    "\n",
    "    return tau, support_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsemaxFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, batch):\n",
    "        \"\"\"sparsemax: normalizing sparse transform\n",
    "        Parameters:\n",
    "            ctx: context object\n",
    "            x (Tensor): shape (N, )\n",
    "            batch: group indicator\n",
    "        Returns:\n",
    "            output (Tensor): same shape as input\n",
    "        \"\"\"\n",
    "        max_val, _ = scatter_max(x, batch)\n",
    "        x -= max_val[batch]\n",
    "        tau, supp_size = _threshold_and_support(x, batch)\n",
    "        output = torch.clamp(x - tau[batch], min=0)\n",
    "        ctx.save_for_backward(supp_size, output, batch)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output, batch = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        return sparsemax(x, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
      "        0.3748])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sparse_attention = Sparsemax()\n",
    "    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n",
    "    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n",
    "    res = sparse_attention(input_x, input_batch)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min')\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers.py\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.8, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(device)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(device)\n",
    "\n",
    "dataset = TUDataset(os.path.join('data', dataset), name=dataset, use_node_attr=True)\n",
    "\n",
    "num_classes = dataset.num_classes\n",
    "num_features = dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.nhid = nhid\n",
    "        self.num_classes = num_classes\n",
    "        self.pooling_ratio = pooling_ratio\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.sample = sample_neighbor\n",
    "        self.sparse = sparse_attention\n",
    "        self.sl = structure_learning\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "        self.conv2 = GCN(self.nhid, self.nhid)\n",
    "        self.conv3 = GCN(self.nhid, self.nhid)\n",
    "\n",
    "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
    "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_attr = None\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = int(len(dataset) * 0.8)\n",
    "num_val = int(len(dataset) * 0.1)\n",
    "num_test = len(dataset) - (num_training + num_val)\n",
    "training_set, validation_set, test_set = random_split(dataset, [num_training, num_val, num_test])\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test(loader):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    loss_test = 0.0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        loss_test += F.nll_loss(out, data.y).item()\n",
    "    return correct / len(loader.dataset), loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    min_loss = 1e10\n",
    "    patience_cnt = 0\n",
    "    val_loss_values = []\n",
    "    best_epoch = 0\n",
    "\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loss_train = 0.0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "        acc_train = correct / len(train_loader.dataset)\n",
    "        acc_val, loss_val = compute_test(val_loader)\n",
    "        print('Epoch: {:04d}'.format(epoch + 1), 'loss_train: {:.6f}'.format(loss_train),\n",
    "              'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
    "              'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
    "\n",
    "        val_loss_values.append(loss_val)\n",
    "        torch.save(model.state_dict(), '{}.pth'.format(epoch))\n",
    "        if val_loss_values[-1] < min_loss:\n",
    "            min_loss = val_loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "\n",
    "        if patience_cnt == patience:\n",
    "            break\n",
    "\n",
    "        files = glob.glob('*.pth')\n",
    "        for f in files:\n",
    "            epoch_nb = int(f.split('.')[0])\n",
    "            if epoch_nb < best_epoch:\n",
    "                os.remove(f)\n",
    "\n",
    "    files = glob.glob('*.pth')\n",
    "    for f in files:\n",
    "        epoch_nb = int(f.split('.')[0])\n",
    "        if epoch_nb > best_epoch:\n",
    "            os.remove(f)\n",
    "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))\n",
    "\n",
    "    return best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 10.407455 acc_train: 0.487261 loss_val: 1.378409 acc_val: 0.641026 time: 0.518736s\n",
      "Epoch: 0002 loss_train: 10.293734 acc_train: 0.597665 loss_val: 1.364082 acc_val: 0.615385 time: 0.991552s\n",
      "Epoch: 0003 loss_train: 10.205101 acc_train: 0.587049 loss_val: 1.348848 acc_val: 0.615385 time: 1.406533s\n",
      "Epoch: 0004 loss_train: 10.124588 acc_train: 0.587049 loss_val: 1.334729 acc_val: 0.615385 time: 1.833610s\n",
      "Epoch: 0005 loss_train: 10.054325 acc_train: 0.587049 loss_val: 1.326167 acc_val: 0.615385 time: 2.253557s\n",
      "Epoch: 0006 loss_train: 10.015354 acc_train: 0.587049 loss_val: 1.319903 acc_val: 0.615385 time: 2.702004s\n",
      "Epoch: 0007 loss_train: 10.003870 acc_train: 0.587049 loss_val: 1.315309 acc_val: 0.615385 time: 3.068483s\n",
      "Epoch: 0008 loss_train: 9.973936 acc_train: 0.587049 loss_val: 1.314553 acc_val: 0.615385 time: 3.548412s\n",
      "Epoch: 0009 loss_train: 9.967011 acc_train: 0.587049 loss_val: 1.313015 acc_val: 0.615385 time: 3.930364s\n",
      "Epoch: 0010 loss_train: 9.940832 acc_train: 0.587049 loss_val: 1.311595 acc_val: 0.615385 time: 4.295382s\n",
      "Epoch: 0011 loss_train: 9.927479 acc_train: 0.587049 loss_val: 1.310078 acc_val: 0.615385 time: 4.650038s\n",
      "Epoch: 0012 loss_train: 9.912819 acc_train: 0.587049 loss_val: 1.307672 acc_val: 0.615385 time: 5.027912s\n",
      "Epoch: 0013 loss_train: 9.890025 acc_train: 0.587049 loss_val: 1.306222 acc_val: 0.615385 time: 5.401204s\n",
      "Epoch: 0014 loss_train: 9.882705 acc_train: 0.587049 loss_val: 1.302600 acc_val: 0.615385 time: 5.775615s\n",
      "Epoch: 0015 loss_train: 9.855252 acc_train: 0.587049 loss_val: 1.300696 acc_val: 0.615385 time: 6.181642s\n",
      "Epoch: 0016 loss_train: 9.816454 acc_train: 0.587049 loss_val: 1.298995 acc_val: 0.615385 time: 6.596531s\n",
      "Epoch: 0017 loss_train: 9.787063 acc_train: 0.587049 loss_val: 1.295247 acc_val: 0.615385 time: 7.111423s\n",
      "Epoch: 0018 loss_train: 9.773893 acc_train: 0.587049 loss_val: 1.291938 acc_val: 0.615385 time: 7.511727s\n",
      "Epoch: 0019 loss_train: 9.726858 acc_train: 0.590234 loss_val: 1.291942 acc_val: 0.615385 time: 7.916920s\n",
      "Epoch: 0020 loss_train: 9.704451 acc_train: 0.589172 loss_val: 1.283756 acc_val: 0.615385 time: 8.335805s\n",
      "Epoch: 0021 loss_train: 9.616967 acc_train: 0.599788 loss_val: 1.283248 acc_val: 0.623932 time: 8.810758s\n",
      "Epoch: 0022 loss_train: 9.561576 acc_train: 0.602972 loss_val: 1.274571 acc_val: 0.623932 time: 9.247303s\n",
      "Epoch: 0023 loss_train: 9.506170 acc_train: 0.609342 loss_val: 1.268805 acc_val: 0.623932 time: 9.679446s\n",
      "Epoch: 0024 loss_train: 9.408285 acc_train: 0.632696 loss_val: 1.266507 acc_val: 0.641026 time: 10.060401s\n",
      "Epoch: 0025 loss_train: 9.344391 acc_train: 0.656051 loss_val: 1.257670 acc_val: 0.606838 time: 10.445500s\n",
      "Epoch: 0026 loss_train: 9.221639 acc_train: 0.676221 loss_val: 1.248131 acc_val: 0.649573 time: 10.838558s\n",
      "Epoch: 0027 loss_train: 9.078782 acc_train: 0.668790 loss_val: 1.235455 acc_val: 0.649573 time: 11.282440s\n",
      "Epoch: 0028 loss_train: 8.971530 acc_train: 0.683652 loss_val: 1.238711 acc_val: 0.709402 time: 11.733719s\n",
      "Epoch: 0029 loss_train: 8.961890 acc_train: 0.734607 loss_val: 1.225442 acc_val: 0.641026 time: 12.206396s\n",
      "Epoch: 0030 loss_train: 8.728921 acc_train: 0.726115 loss_val: 1.200586 acc_val: 0.700855 time: 12.618512s\n",
      "Epoch: 0031 loss_train: 8.598560 acc_train: 0.730361 loss_val: 1.191320 acc_val: 0.700855 time: 12.997442s\n",
      "Epoch: 0032 loss_train: 8.440616 acc_train: 0.760085 loss_val: 1.174975 acc_val: 0.709402 time: 13.376459s\n",
      "Epoch: 0033 loss_train: 8.240191 acc_train: 0.769639 loss_val: 1.156743 acc_val: 0.726496 time: 13.772757s\n",
      "Epoch: 0034 loss_train: 8.122625 acc_train: 0.766454 loss_val: 1.143072 acc_val: 0.700855 time: 14.156746s\n",
      "Epoch: 0035 loss_train: 8.027650 acc_train: 0.759023 loss_val: 1.126380 acc_val: 0.709402 time: 14.577690s\n",
      "Epoch: 0036 loss_train: 7.887943 acc_train: 0.772824 loss_val: 1.150547 acc_val: 0.683761 time: 14.954810s\n",
      "Epoch: 0037 loss_train: 7.854975 acc_train: 0.764331 loss_val: 1.110509 acc_val: 0.717949 time: 15.379306s\n",
      "Epoch: 0038 loss_train: 7.709308 acc_train: 0.773885 loss_val: 1.106930 acc_val: 0.786325 time: 15.834676s\n",
      "Epoch: 0039 loss_train: 7.681411 acc_train: 0.783439 loss_val: 1.091020 acc_val: 0.735043 time: 16.305650s\n",
      "Epoch: 0040 loss_train: 7.562623 acc_train: 0.777070 loss_val: 1.074525 acc_val: 0.717949 time: 16.728501s\n",
      "Epoch: 0041 loss_train: 7.490091 acc_train: 0.783439 loss_val: 1.061319 acc_val: 0.769231 time: 17.105803s\n",
      "Epoch: 0042 loss_train: 7.429185 acc_train: 0.782378 loss_val: 1.098472 acc_val: 0.717949 time: 17.474468s\n",
      "Epoch: 0043 loss_train: 7.528086 acc_train: 0.769639 loss_val: 1.061954 acc_val: 0.777778 time: 17.869612s\n",
      "Epoch: 0044 loss_train: 7.329761 acc_train: 0.777070 loss_val: 1.068092 acc_val: 0.777778 time: 18.253176s\n",
      "Epoch: 0045 loss_train: 7.284270 acc_train: 0.785563 loss_val: 1.058011 acc_val: 0.769231 time: 18.636232s\n",
      "Epoch: 0046 loss_train: 7.239642 acc_train: 0.784501 loss_val: 1.059743 acc_val: 0.769231 time: 19.112392s\n",
      "Epoch: 0047 loss_train: 7.265311 acc_train: 0.781316 loss_val: 1.065395 acc_val: 0.760684 time: 19.521270s\n",
      "Epoch: 0048 loss_train: 7.233727 acc_train: 0.784501 loss_val: 1.074592 acc_val: 0.769231 time: 19.972674s\n",
      "Epoch: 0049 loss_train: 7.191637 acc_train: 0.787686 loss_val: 1.094875 acc_val: 0.735043 time: 20.419211s\n",
      "Epoch: 0050 loss_train: 7.167952 acc_train: 0.791932 loss_val: 1.071406 acc_val: 0.769231 time: 20.840760s\n",
      "Epoch: 0051 loss_train: 7.173928 acc_train: 0.792994 loss_val: 1.067089 acc_val: 0.769231 time: 21.246675s\n",
      "Epoch: 0052 loss_train: 7.063790 acc_train: 0.795117 loss_val: 1.088336 acc_val: 0.760684 time: 21.660517s\n",
      "Epoch: 0053 loss_train: 7.028439 acc_train: 0.796178 loss_val: 1.074010 acc_val: 0.752137 time: 22.062562s\n",
      "Epoch: 0054 loss_train: 7.023258 acc_train: 0.786624 loss_val: 1.069409 acc_val: 0.760684 time: 22.458135s\n",
      "Epoch: 0055 loss_train: 6.946802 acc_train: 0.791932 loss_val: 1.130761 acc_val: 0.743590 time: 22.859209s\n",
      "Epoch: 0056 loss_train: 7.019573 acc_train: 0.786624 loss_val: 1.078838 acc_val: 0.760684 time: 23.265115s\n",
      "Epoch: 0057 loss_train: 6.939854 acc_train: 0.791932 loss_val: 1.088930 acc_val: 0.752137 time: 23.657060s\n",
      "Epoch: 0058 loss_train: 6.878782 acc_train: 0.804671 loss_val: 1.086818 acc_val: 0.752137 time: 24.058435s\n",
      "Epoch: 0059 loss_train: 6.842486 acc_train: 0.796178 loss_val: 1.077955 acc_val: 0.769231 time: 24.457376s\n",
      "Epoch: 0060 loss_train: 6.891592 acc_train: 0.787686 loss_val: 1.083563 acc_val: 0.752137 time: 24.850024s\n",
      "Epoch: 0061 loss_train: 6.787822 acc_train: 0.794055 loss_val: 1.084367 acc_val: 0.735043 time: 25.265576s\n",
      "Epoch: 0062 loss_train: 6.790039 acc_train: 0.792994 loss_val: 1.095889 acc_val: 0.752137 time: 25.625402s\n",
      "Epoch: 0063 loss_train: 6.755458 acc_train: 0.790870 loss_val: 1.093028 acc_val: 0.735043 time: 25.986062s\n",
      "Epoch: 0064 loss_train: 6.782484 acc_train: 0.784501 loss_val: 1.099387 acc_val: 0.752137 time: 26.354337s\n",
      "Epoch: 0065 loss_train: 6.733547 acc_train: 0.795117 loss_val: 1.094915 acc_val: 0.743590 time: 26.738535s\n",
      "Epoch: 0066 loss_train: 6.677385 acc_train: 0.797240 loss_val: 1.088963 acc_val: 0.752137 time: 27.120086s\n",
      "Epoch: 0067 loss_train: 6.629001 acc_train: 0.802548 loss_val: 1.077982 acc_val: 0.777778 time: 27.640115s\n",
      "Epoch: 0068 loss_train: 6.623696 acc_train: 0.804671 loss_val: 1.090515 acc_val: 0.743590 time: 28.062304s\n",
      "Epoch: 0069 loss_train: 6.633326 acc_train: 0.802548 loss_val: 1.076267 acc_val: 0.743590 time: 28.479656s\n",
      "Epoch: 0070 loss_train: 6.629284 acc_train: 0.796178 loss_val: 1.073844 acc_val: 0.752137 time: 29.003199s\n",
      "Epoch: 0071 loss_train: 6.619927 acc_train: 0.796178 loss_val: 1.079405 acc_val: 0.760684 time: 29.381272s\n",
      "Epoch: 0072 loss_train: 6.617269 acc_train: 0.796178 loss_val: 1.090094 acc_val: 0.769231 time: 29.748336s\n",
      "Epoch: 0073 loss_train: 6.563961 acc_train: 0.801486 loss_val: 1.082276 acc_val: 0.743590 time: 30.125244s\n",
      "Epoch: 0074 loss_train: 6.558794 acc_train: 0.798301 loss_val: 1.082815 acc_val: 0.743590 time: 30.524447s\n",
      "Epoch: 0075 loss_train: 6.615310 acc_train: 0.797240 loss_val: 1.084526 acc_val: 0.769231 time: 30.912240s\n",
      "Epoch: 0076 loss_train: 6.453997 acc_train: 0.800425 loss_val: 1.104607 acc_val: 0.752137 time: 31.303817s\n",
      "Epoch: 0077 loss_train: 6.613473 acc_train: 0.796178 loss_val: 1.097313 acc_val: 0.752137 time: 31.746193s\n",
      "Epoch: 0078 loss_train: 6.620708 acc_train: 0.798301 loss_val: 1.086425 acc_val: 0.769231 time: 32.130253s\n",
      "Epoch: 0079 loss_train: 6.452977 acc_train: 0.806794 loss_val: 1.090394 acc_val: 0.769231 time: 32.523417s\n",
      "Epoch: 0080 loss_train: 6.557847 acc_train: 0.797240 loss_val: 1.075428 acc_val: 0.735043 time: 32.909416s\n",
      "Epoch: 0081 loss_train: 6.461104 acc_train: 0.805732 loss_val: 1.092014 acc_val: 0.769231 time: 33.281389s\n",
      "Epoch: 0082 loss_train: 6.508157 acc_train: 0.806794 loss_val: 1.106893 acc_val: 0.760684 time: 33.707896s\n",
      "Epoch: 0083 loss_train: 6.488577 acc_train: 0.809979 loss_val: 1.129977 acc_val: 0.769231 time: 34.130131s\n",
      "Epoch: 0084 loss_train: 6.422371 acc_train: 0.807856 loss_val: 1.080451 acc_val: 0.735043 time: 34.577094s\n",
      "Epoch: 0085 loss_train: 6.394749 acc_train: 0.800425 loss_val: 1.085680 acc_val: 0.752137 time: 35.006129s\n",
      "Epoch: 0086 loss_train: 6.381233 acc_train: 0.802548 loss_val: 1.088217 acc_val: 0.769231 time: 35.455305s\n",
      "Epoch: 0087 loss_train: 6.429856 acc_train: 0.802548 loss_val: 1.077934 acc_val: 0.752137 time: 35.865220s\n",
      "Epoch: 0088 loss_train: 6.390632 acc_train: 0.806794 loss_val: 1.103442 acc_val: 0.769231 time: 36.279033s\n",
      "Epoch: 0089 loss_train: 6.438407 acc_train: 0.808917 loss_val: 1.075764 acc_val: 0.769231 time: 36.698468s\n",
      "Epoch: 0090 loss_train: 6.337817 acc_train: 0.811040 loss_val: 1.081843 acc_val: 0.735043 time: 37.153662s\n",
      "Epoch: 0091 loss_train: 6.325496 acc_train: 0.815287 loss_val: 1.087664 acc_val: 0.777778 time: 37.596106s\n",
      "Epoch: 0092 loss_train: 6.253876 acc_train: 0.816348 loss_val: 1.092241 acc_val: 0.769231 time: 38.014172s\n",
      "Epoch: 0093 loss_train: 6.230837 acc_train: 0.818471 loss_val: 1.097098 acc_val: 0.769231 time: 38.432101s\n",
      "Epoch: 0094 loss_train: 6.192620 acc_train: 0.815287 loss_val: 1.094192 acc_val: 0.777778 time: 38.835127s\n",
      "Epoch: 0095 loss_train: 6.175211 acc_train: 0.817410 loss_val: 1.102202 acc_val: 0.752137 time: 39.205339s\n",
      "Epoch: 0096 loss_train: 6.141860 acc_train: 0.818471 loss_val: 1.102011 acc_val: 0.743590 time: 39.587241s\n",
      "Epoch: 0097 loss_train: 6.124465 acc_train: 0.829087 loss_val: 1.104813 acc_val: 0.743590 time: 39.964080s\n",
      "Epoch: 0098 loss_train: 6.260724 acc_train: 0.811040 loss_val: 1.109693 acc_val: 0.760684 time: 40.342072s\n",
      "Epoch: 0099 loss_train: 6.198995 acc_train: 0.818471 loss_val: 1.154679 acc_val: 0.743590 time: 40.714197s\n",
      "Epoch: 0100 loss_train: 6.241033 acc_train: 0.815287 loss_val: 1.123222 acc_val: 0.769231 time: 41.089504s\n",
      "Epoch: 0101 loss_train: 6.090741 acc_train: 0.818471 loss_val: 1.107992 acc_val: 0.769231 time: 41.546186s\n",
      "Epoch: 0102 loss_train: 6.089609 acc_train: 0.822718 loss_val: 1.116701 acc_val: 0.769231 time: 41.919182s\n",
      "Epoch: 0103 loss_train: 6.084804 acc_train: 0.816348 loss_val: 1.108164 acc_val: 0.760684 time: 42.297102s\n",
      "Epoch: 0104 loss_train: 6.079938 acc_train: 0.822718 loss_val: 1.111343 acc_val: 0.743590 time: 42.714405s\n",
      "Epoch: 0105 loss_train: 6.098433 acc_train: 0.817410 loss_val: 1.112808 acc_val: 0.760684 time: 43.082925s\n",
      "Epoch: 0106 loss_train: 5.960158 acc_train: 0.825902 loss_val: 1.105673 acc_val: 0.760684 time: 43.573879s\n",
      "Epoch: 0107 loss_train: 5.962155 acc_train: 0.825902 loss_val: 1.107151 acc_val: 0.760684 time: 43.989469s\n",
      "Epoch: 0108 loss_train: 5.956675 acc_train: 0.826964 loss_val: 1.117087 acc_val: 0.760684 time: 44.482101s\n",
      "Epoch: 0109 loss_train: 5.920743 acc_train: 0.819533 loss_val: 1.095398 acc_val: 0.769231 time: 44.902114s\n",
      "Epoch: 0110 loss_train: 5.969065 acc_train: 0.826964 loss_val: 1.093632 acc_val: 0.769231 time: 45.336022s\n",
      "Epoch: 0111 loss_train: 5.958039 acc_train: 0.825902 loss_val: 1.131089 acc_val: 0.752137 time: 45.737023s\n",
      "Epoch: 0112 loss_train: 5.993552 acc_train: 0.824841 loss_val: 1.089700 acc_val: 0.794872 time: 46.160174s\n",
      "Epoch: 0113 loss_train: 6.030872 acc_train: 0.824841 loss_val: 1.093455 acc_val: 0.760684 time: 46.612205s\n",
      "Epoch: 0114 loss_train: 5.948289 acc_train: 0.821656 loss_val: 1.090033 acc_val: 0.760684 time: 47.042111s\n",
      "Epoch: 0115 loss_train: 5.946519 acc_train: 0.829087 loss_val: 1.072788 acc_val: 0.786325 time: 47.466285s\n",
      "Epoch: 0116 loss_train: 6.001823 acc_train: 0.821656 loss_val: 1.071545 acc_val: 0.786325 time: 47.920634s\n",
      "Epoch: 0117 loss_train: 5.876606 acc_train: 0.821656 loss_val: 1.149581 acc_val: 0.726496 time: 48.293417s\n",
      "Epoch: 0118 loss_train: 6.059328 acc_train: 0.825902 loss_val: 1.060118 acc_val: 0.777778 time: 48.668716s\n",
      "Epoch: 0119 loss_train: 5.843466 acc_train: 0.828025 loss_val: 1.079849 acc_val: 0.777778 time: 49.042320s\n",
      "Epoch: 0120 loss_train: 5.825723 acc_train: 0.828025 loss_val: 1.076974 acc_val: 0.777778 time: 49.464039s\n",
      "Epoch: 0121 loss_train: 5.881490 acc_train: 0.828025 loss_val: 1.084582 acc_val: 0.777778 time: 49.855957s\n",
      "Epoch: 0122 loss_train: 5.756991 acc_train: 0.831210 loss_val: 1.098209 acc_val: 0.777778 time: 50.239538s\n",
      "Epoch: 0123 loss_train: 5.932729 acc_train: 0.820594 loss_val: 1.080787 acc_val: 0.786325 time: 50.607112s\n",
      "Epoch: 0124 loss_train: 5.961151 acc_train: 0.819533 loss_val: 1.081694 acc_val: 0.777778 time: 50.986363s\n",
      "Epoch: 0125 loss_train: 5.846162 acc_train: 0.825902 loss_val: 1.075581 acc_val: 0.777778 time: 51.433985s\n",
      "Epoch: 0126 loss_train: 5.858686 acc_train: 0.816348 loss_val: 1.099064 acc_val: 0.717949 time: 51.880761s\n",
      "Epoch: 0127 loss_train: 5.875635 acc_train: 0.828025 loss_val: 1.074048 acc_val: 0.769231 time: 52.291512s\n",
      "Epoch: 0128 loss_train: 5.689355 acc_train: 0.833333 loss_val: 1.098942 acc_val: 0.752137 time: 52.732388s\n",
      "Epoch: 0129 loss_train: 5.673340 acc_train: 0.836518 loss_val: 1.094575 acc_val: 0.760684 time: 53.169261s\n",
      "Epoch: 0130 loss_train: 5.735400 acc_train: 0.831210 loss_val: 1.110460 acc_val: 0.752137 time: 53.591453s\n",
      "Epoch: 0131 loss_train: 5.687843 acc_train: 0.841826 loss_val: 1.140313 acc_val: 0.743590 time: 53.991511s\n",
      "Epoch: 0132 loss_train: 5.769658 acc_train: 0.837580 loss_val: 1.112455 acc_val: 0.760684 time: 54.406527s\n",
      "Epoch: 0133 loss_train: 5.859644 acc_train: 0.819533 loss_val: 1.115109 acc_val: 0.752137 time: 54.806156s\n",
      "Epoch: 0134 loss_train: 5.661716 acc_train: 0.830149 loss_val: 1.111298 acc_val: 0.752137 time: 55.313277s\n",
      "Epoch: 0135 loss_train: 5.830031 acc_train: 0.823779 loss_val: 1.121354 acc_val: 0.769231 time: 55.733003s\n",
      "Epoch: 0136 loss_train: 5.607222 acc_train: 0.840764 loss_val: 1.129832 acc_val: 0.752137 time: 56.164438s\n",
      "Epoch: 0137 loss_train: 5.615391 acc_train: 0.834395 loss_val: 1.115586 acc_val: 0.760684 time: 56.531486s\n",
      "Epoch: 0138 loss_train: 5.658191 acc_train: 0.833333 loss_val: 1.174083 acc_val: 0.700855 time: 56.904612s\n",
      "Epoch: 0139 loss_train: 5.796613 acc_train: 0.832272 loss_val: 1.117650 acc_val: 0.777778 time: 57.301277s\n",
      "Epoch: 0140 loss_train: 5.617440 acc_train: 0.843949 loss_val: 1.113232 acc_val: 0.769231 time: 57.718341s\n",
      "Epoch: 0141 loss_train: 5.508641 acc_train: 0.849257 loss_val: 1.100771 acc_val: 0.777778 time: 58.074428s\n",
      "Epoch: 0142 loss_train: 5.537290 acc_train: 0.845011 loss_val: 1.110618 acc_val: 0.760684 time: 58.482821s\n",
      "Epoch: 0143 loss_train: 5.665635 acc_train: 0.835456 loss_val: 1.099578 acc_val: 0.786325 time: 58.908065s\n",
      "Epoch: 0144 loss_train: 5.673759 acc_train: 0.838641 loss_val: 1.096079 acc_val: 0.777778 time: 59.299212s\n",
      "Epoch: 0145 loss_train: 5.544601 acc_train: 0.843949 loss_val: 1.085997 acc_val: 0.786325 time: 59.659309s\n",
      "Optimization Finished! Total time elapsed: 59.756373\n",
      "Test set results, loss = 0.960404, accuracy = 0.781513\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Model training\n",
    "    best_model = train()\n",
    "    # Restore best model for test set\n",
    "    model.load_state_dict(torch.load('{}.pth'.format(best_model)))\n",
    "    test_acc, test_loss = compute_test(test_loader)\n",
    "    print('Test set results, loss = {:.6f}, accuracy = {:.6f}'.format(test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
